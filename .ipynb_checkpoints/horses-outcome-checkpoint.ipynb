{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebooks that inspired this work**\n",
    "\n",
    "* https://www.kaggle.com/code/kimtaehun/eda-and-baseline-with-multiple-models\n",
    "* https://www.kaggle.com/code/yaaangzhou/playground-s3-e22-eda-modeling#3.-Preprocessing-and-Features-Selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ideias**\n",
    "\n",
    "* lesion_3 should be dropped (chi-square test)\n",
    "* Temperature pow(2)\n",
    "* get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import re as re\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import math\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "%matplotlib inline\n",
    "tqdm.pandas()\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFF9ED\",\n",
    "    \"figure.facecolor\": \"#FFF9ED\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "\n",
    "sns.set(rc=rc)\n",
    "\n",
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('playground-series-s3e22/train.csv')\n",
    "original = pd.read_csv(\"horse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary table function\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "def summary(df):\n",
    "    print(f'data shape: {df.shape}')\n",
    "    summ = pd.DataFrame(df.dtypes, columns=['data type'])\n",
    "    summ['#missing'] = df.isnull().sum().values \n",
    "    summ['%missing'] = df.isnull().sum().values / len(df) * 100\n",
    "    summ['#unique'] = df.nunique().values\n",
    "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
    "    summ['min'] = desc['min'].values\n",
    "    summ['max'] = desc['max'].values\n",
    "    summ['average'] = desc['mean'].values\n",
    "    summ['standard_deviation'] = desc['std'].values\n",
    "    summ['first value'] = df.loc[0].values\n",
    "    summ['second value'] = df.loc[1].values\n",
    "    summ['third value'] = df.loc[2].values\n",
    "    \n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(train).style.background_gradient(cmap='YlOrBr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color='#DEB887'>ðŸ’¡ About Dataset:</font></h3>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Objective</b>: The dataset aims to predict whether a horse can survive based on past medical conditions.</li>\n",
    "    <li><b>Outcome</b>: Noted by the \"outcome\" variable. Possibilities include: lived, died, was euthanized.</li>\n",
    "    <li><b>Missing Values</b>: The dataset contains a significant number of NA values, emphasizing the importance of data imputation in the preprocessing steps.</li>\n",
    "    <li><b>Attributes</b>: The dataset provides a mix of categorical and linear (numeric) variables, covering a variety of clinical measurements, subjective judgments, and explicit medical findings. In total, there are 28 attributes to consider, with the 'outcome' being the target variable.</li>\n",
    "    <li><b>Special Notes</b>: Some attributes, such as 'pain', should not be treated as ordered or discrete. Others, like 'type of lesion', break down into several sub-categories.</li>\n",
    "    <li><b>Contextual Information</b>: Certain parameters, like 'rectal temperature' and 'pulse', offer specific context into the condition of the horse, such as potential infections or shock.</li>\n",
    "    <li><b>Significant Attributes</b>: Parameters like 'abdominal distension' and 'nasogastric reflux' provide crucial indications about the horse's health and potential need for surgical interventions.</li>\n",
    "</ul>\n",
    "\n",
    "This summary offers a detailed initial understanding of the dataset's characteristics, assisting in making informed decisions in the subsequent analysis and modeling steps.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color='#DEB887'>ðŸ’¡ Dataset Attributes Description:</font></h3>\n",
    "\n",
    "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Attribute</th>\n",
    "            <th>Description</th>\n",
    "            <th>Values</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>surgery?</td>\n",
    "            <td>Whether the horse had surgery</td>\n",
    "            <td>1 = Yes, 2 = No</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Age</td>\n",
    "            <td>Age category of the horse</td>\n",
    "            <td>1 = Adult, 2 = Young (&lt; 6 months)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Hospital Number</td>\n",
    "            <td>Case number assigned to the horse</td>\n",
    "            <td>Numeric ID</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>rectal temperature</td>\n",
    "            <td>Temperature in degrees celsius</td>\n",
    "            <td>Linear</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>pulse</td>\n",
    "            <td>Heart rate in beats per minute</td>\n",
    "            <td>Linear</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>respiratory rate</td>\n",
    "            <td>Rate of respiration</td>\n",
    "            <td>Linear</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>temperature of extremities</td>\n",
    "            <td>Indication of peripheral circulation</td>\n",
    "            <td>1 = Normal, 2 = Warm, 3 = Cool, 4 = Cold</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>peripheral pulse</td>\n",
    "            <td>Subjective assessment of peripheral pulse</td>\n",
    "            <td>1 = Normal, 2 = Increased, 3 = Reduced, 4 = Absent</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>mucous membranes</td>\n",
    "            <td>Measurement of color of mucous membranes</td>\n",
    "            <td>1-6 as described in the given data</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>capillary refill time</td>\n",
    "            <td>Clinical judgment of capillary refill time</td>\n",
    "            <td>1 = &lt; 3 seconds, 2 = &gt;= 3 seconds</td>\n",
    "        </tr>\n",
    "        <!-- ... Rest of the rows for each attribute ... -->\n",
    "        <tr>\n",
    "            <td>abdominocentesis appearance</td>\n",
    "            <td>Appearance of fluid from abdominocentesis</td>\n",
    "            <td>1 = Clear, 2 = Cloudy, 3 = Serosanguinous</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>abdomcentesis total protein</td>\n",
    "            <td>Total protein from abdominocentesis</td>\n",
    "            <td>Linear (gms/dL)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>outcome</td>\n",
    "            <td>Final outcome for the horse</td>\n",
    "            <td>1 = Lived, 2 = Died, 3 = Euthanized</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>surgical lesion?</td>\n",
    "            <td>If the lesion was surgical</td>\n",
    "            <td>1 = Yes, 2 = No</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>type of lesion</td>\n",
    "            <td>Type of lesion identified</td>\n",
    "            <td>Comprehensive description given (Multiple layers)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>cp_data</td>\n",
    "            <td>Presence of pathology data for the case</td>\n",
    "            <td>1 = Yes, 2 = No</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ðŸ“Š Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.concat([train, original], ignore_index = True)\n",
    "\n",
    "def plot_count(df: pd.core.frame.DataFrame, col: str, title_name: str='Train') -> None:\n",
    "    # Set background color\n",
    "    plt.rcParams['figure.facecolor'] = '#FFFAF0'\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    s1 = df[col].value_counts()\n",
    "    N = len(s1)\n",
    "\n",
    "    outer_sizes = s1\n",
    "    inner_sizes = s1/N\n",
    "\n",
    "    outer_colors = ['#9E3F00', '#eb5e00', '#ff781f']\n",
    "    inner_colors = ['#ff6905', '#ff8838', '#ffa66b']\n",
    "\n",
    "    ax[0].pie(\n",
    "        outer_sizes,colors=outer_colors, \n",
    "        labels=s1.index.tolist(), \n",
    "        startangle=90, frame=True, radius=1.3, \n",
    "        explode=([0.05]*(N-1) + [.3]),\n",
    "        wedgeprops={'linewidth' : 1, 'edgecolor' : 'white'}, \n",
    "        textprops={'fontsize': 12, 'weight': 'bold'}\n",
    "    )\n",
    "\n",
    "    textprops = {\n",
    "        'size': 13, \n",
    "        'weight': 'bold', \n",
    "        'color': 'white'\n",
    "    }\n",
    "\n",
    "    ax[0].pie(\n",
    "        inner_sizes, colors=inner_colors,\n",
    "        radius=1, startangle=90,\n",
    "        autopct='%1.f%%', explode=([.1]*(N-1) + [.3]),\n",
    "        pctdistance=0.8, textprops=textprops\n",
    "    )\n",
    "\n",
    "    center_circle = plt.Circle((0,0), .68, color='black', fc='white', linewidth=0)\n",
    "    ax[0].add_artist(center_circle)\n",
    "\n",
    "    x = s1\n",
    "    y = s1.index.tolist()\n",
    "    sns.barplot(\n",
    "        x=x, y=y, ax=ax[1],\n",
    "        palette='YlOrBr_r', orient='horizontal'\n",
    "    )\n",
    "\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].tick_params(\n",
    "        axis='x',         \n",
    "        which='both',      \n",
    "        bottom=False,      \n",
    "        labelbottom=False\n",
    "    )\n",
    "\n",
    "    for i, v in enumerate(s1):\n",
    "        ax[1].text(v, i+0.1, str(v), color='black', fontweight='bold', fontsize=12)\n",
    "\n",
    "    plt.setp(ax[1].get_yticklabels(), fontweight=\"bold\")\n",
    "    plt.setp(ax[1].get_xticklabels(), fontweight=\"bold\")\n",
    "    ax[1].set_xlabel(col, fontweight=\"bold\", color='black')\n",
    "    ax[1].set_ylabel('count', fontweight=\"bold\", color='black')\n",
    "\n",
    "    f.suptitle(f'{title_name}', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count(df, 'outcome', 'Target Variable(Outcome) Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique value counts for each column\n",
    "unique_counts = train.nunique()\n",
    "\n",
    "# Threshold to distinguish continuous and categorical\n",
    "threshold = 10\n",
    "\n",
    "continuous_vars = unique_counts[unique_counts > threshold].index.tolist()\n",
    "categorical_vars = unique_counts[unique_counts <= threshold].index.tolist()\n",
    "\n",
    "# Removing the 'outcome' from categorical since it's our target variable\n",
    "if 'outcome' in categorical_vars:\n",
    "    categorical_vars.remove('outcome')\n",
    "if 'id' in continuous_vars:\n",
    "    continuous_vars.remove('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ðŸ“Š Variables distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuos_vars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, len(continuous_vars) * 2.5))\n",
    "\n",
    "for idx, column in enumerate(continuous_vars):\n",
    "    # Plotting for outcome\n",
    "    plt.subplot(len(continuous_vars), 2, idx*2+1)\n",
    "    sns.histplot(x=column, hue=\"outcome\", data=df, bins=30, kde=True, palette='YlOrRd')\n",
    "    plt.title(f\"{column} Distribution for outcome\")\n",
    "    plt.ylim(0, df[column].value_counts().max() + 10)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, len(categorical_vars)*2.5))\n",
    "\n",
    "for idx, column in enumerate(categorical_vars):\n",
    "    plt.subplot(len(categorical_vars)//2 + len(categorical_vars) % 2, 2, idx+1)\n",
    "    sns.countplot(x=column, hue=\"outcome\", data=df, palette='YlOrRd')\n",
    "    plt.title(f\"{column} Countplot by outcome\")\n",
    "    plt.ylim(0, df[column].value_counts().max() + 10)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
    "<h3 align=\"left\"><font color='#DEB887'>ðŸ’¡ Recommendations on Variables:</font></h3>\n",
    "    \n",
    "* A plethora of variables seem to manifest a substantial distinctive power in the dataset. However, the untreated missing values can obscure their true potential or influence. Addressing these missing values is of paramount importance for an accurate assessment and leveraging of these variables.\n",
    "\n",
    "* Special attention should be paid to variables like 'lesion'. A meticulous preprocessing approach for such variables can not only enhance their utility but also prevent potential pitfalls or biases in the model's predictions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming these are your categorical variables\n",
    "categorical_vars = ['surgery', 'age', 'temp_of_extremities', 'peripheral_pulse', \n",
    "                    'mucous_membrane', 'capillary_refill_time', 'pain', 'peristalsis', \n",
    "                    'abdominal_distention', 'nasogastric_tube', 'nasogastric_reflux', \n",
    "                    'rectal_exam_feces', 'abdomen', 'abdomo_appearance', 'surgical_lesion', \n",
    "                    'cp_data']\n",
    "\n",
    "def encode(df, cat_cols):\n",
    "\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Label encode categorical columns\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for column in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "        \n",
    "    return df_encoded, label_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
    "\n",
    "<h3 align=\"left\"><font color='#DEB887'>ðŸ’¡ About Dataset:</font></h3>\n",
    "\n",
    "<ul>\n",
    "    Using label encoding to calculate the correlation for categorical variables has some caveats:\n",
    "    <li><b>Arbitrary Ordering: </b> Label encoding assigns arbitrary numeric values to categorical data. These numbers might not have any inherent relationship with the original meaning of the data. Hence, when interpreting the correlation coefficient, this must be taken into account.</li>\n",
    "    <li><b>Interpreting Correlation: </b> The correlation among label-encoded categorical variables might not accurately reflect the actual relationships between the variables. Especially if the order of the labels does not align with the true meaning of the data, caution in interpretation is required.</li>\n",
    "    <li><b>Alternative Methods: </b>There are other statistical methods or techniques (e.g., Chi-squared test) that can be used to understand relationships between categorical data.</li>\n",
    "   \n",
    "</ul>\n",
    "\n",
    "In summary, caution is needed when interpreting the correlation of label-encoded categorical variables. This approach might not represent the true relationship in the data, so it's better used as a reference in data exploration or preprocessing rather than for direct interpretation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'outcome'\n",
    "\n",
    "def process_data(df):\n",
    "    \n",
    "    df[\"pain\"] = df[\"pain\"].replace('slight', 'moderate')\n",
    "    df[\"peristalsis\"] = df[\"peristalsis\"].replace('distend_small', 'normal')\n",
    "    df[\"rectal_exam_feces\"] = df[\"rectal_exam_feces\"].replace('serosanguious', 'absent')\n",
    "    df[\"nasogastric_reflux\"] = df[\"nasogastric_reflux\"].replace('slight', 'none')\n",
    "    \n",
    "    # df['temp_of_extremities_status']=df['temp_of_extremities'].apply(lambda x:1 if x=='normal' else 0)\n",
    "    # df['rectal_temp_status']=df['rectal_temp'].apply(lambda x:1 if 37.5<=x<=38.6 else 0)\n",
    "    # df['temp_status']=np.where((df['temp_of_extremities_status']==1)&(df['rectal_temp_status']==1),1,0)\n",
    "    \n",
    "    for col in categorical_vars:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    for col in continuous_vars:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    \n",
    "    features = df.drop(columns=['id', 'lesion_3']).columns.tolist()\n",
    "    df_x = df[features]\n",
    "    \n",
    "    df_encoded, label_encoders = encode(df_x, categorical_vars)\n",
    "    \n",
    "    return df_encoded, features, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed, all_features, labels = process_data(df)\n",
    "all_features.remove('outcome')\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_processed['outcome'] = le.fit_transform(data_processed['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_processed[all_features]\n",
    "y = data_processed[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def eval_model(model, vars, target, scoring='f1_micro', folds=7):\n",
    "    \n",
    "    cv_scores = cross_val_score(model,vars,target,scoring=scoring,cv=folds)\n",
    "    scores = np.mean(cv_scores)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "xgb_md = XGBClassifier(objective='multi:softmax', num_class=3, )\n",
    "lgb_md = LGBMClassifier(objective = 'multiclass')\n",
    "cat_md = CatBoostClassifier(loss_function='MultiClass', eval_metric='TotalF1:average=Macro', verbose=0)\n",
    "hist_md = HistGradientBoostingClassifier(loss='categorical_crossentropy')\n",
    "gbc_md = GradientBoostingClassifier()\n",
    "\n",
    "models = {\n",
    "    'xgb': xgb_md,\n",
    "    'lgb': lgb_md,\n",
    "    'cat': cat_md,\n",
    "    'hist': hist_md,\n",
    "    'gbc': gbc_md,\n",
    "}\n",
    "\n",
    "models_score = {}\n",
    "\n",
    "# test models\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\033[1;34mTraining {model_name} models\\033[0m')  # Blue for model names\n",
    "    \n",
    "    score = eval_model(model, X, y)\n",
    "    models_score[model_name] = score\n",
    "    \n",
    "    print(f'\\033[1;32mMean score: {score}\\033[0m')  # Green for scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optuna Time my dear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import create_study, logging\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.integration import XGBoostPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "\n",
    "    param = {\n",
    "            \n",
    "        'objective':'multi:softmax',\n",
    "        'num_class': 3,\n",
    "        'lambda': trial.suggest_loguniform(\"lambda\", 1e-8, 2.0),\n",
    "        'alpha': trial.suggest_loguniform(\"alpha\", 1e-8, 2.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0, 1),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0, 1),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.008, 0.95),\n",
    "        'num_boost_round': trial.suggest_int('num_boost_round', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 30),\n",
    "        'min_child_weight': trial.suggest_loguniform('min_child_weight',1e-10, 1e10),\n",
    "\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = XGBClassifier(**param)\n",
    "    score = eval_model(model, X, y, folds=5)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "study = create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = study.best_trial.params\n",
    "\n",
    "model = XGBClassifier(**tuned_params)#best model so far\n",
    "print(eval_model(model, X, y, folds=5))\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"playground-series-s3e22/sample_submission.csv\")\n",
    "test = pd.read_csv(\"playground-series-s3e22/test.csv\")\n",
    "\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed, all_test_features, test_labels = process_data(test)\n",
    "\n",
    "pred = model.predict(test_processed)\n",
    "pred = le.inverse_transform(pred)\n",
    "result['outcome'] = pred\n",
    "result.to_csv('xgb-tuned3-_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ðŸ“Š evaluation and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #code for evaulation and comparision + feature importance for each models.\n",
    "\n",
    "# for model_name, model_dict in best_models.items():\n",
    "#     for target in target_cols:\n",
    "#         print(f'\\033[1;34;4mVisualization for {model_name} {target}\\033[0m')  # Blue with underline for model and target\n",
    "        \n",
    "#         best_val_preds = model_dict[target].predict(df[features])\n",
    "#         f1 = f1_score(df[target], best_val_preds, average='micro')\n",
    "#         print(f'\\033[1;35mMicro-averaged F1-Score: {f1:.5f}\\033[0m')  # Purple for F1 scores\n",
    "\n",
    "#         # Confusion Matrix Visualization\n",
    "#         show_confusion(best_val_preds, df[target])\n",
    "\n",
    "#         # Feature Importance Visualization\n",
    "#         f_imp_df = pd.DataFrame({'feature': features, 'avg_imp': model_dict[target].feature_importances_})\n",
    "#         f_importance_plot(f_imp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "def f_importance_plot(f_imp):\n",
    "    fig = plt.figure(figsize=(12, 0.20*len(f_imp)))\n",
    "    plt.title('Feature importances', size=16, y=1.05, \n",
    "              fontweight='bold', color='#444444')\n",
    "    a = sns.barplot(data=f_imp, x='avg_imp', y='feature', \n",
    "                    palette='YlOrBr_r', linestyle=\"-\", \n",
    "                    linewidth=0.5, edgecolor=\"black\")\n",
    "    plt.xlabel('')\n",
    "    plt.xticks([])\n",
    "    plt.ylabel('')\n",
    "    plt.yticks(size=11, color='#444444')\n",
    "    \n",
    "    for j in ['right', 'top', 'bottom']:\n",
    "        a.spines[j].set_visible(False)\n",
    "    for j in ['left']:\n",
    "        a.spines[j].set_linewidth(0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def show_confusion(y_pred, y_true):\n",
    "    # Assuming y_true and y_pred are numpy arrays\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
    "<h3 align=\"left\"><font color='#DEB887'>ðŸ’¡ Conclusions:</font></h3>\n",
    "\n",
    "* This dataset warrants a thorough preprocessing approach for significant improvements. However, for illustrative purposes, I carried out a very basic preprocessing. This has likely contributed to the less-than-ideal micro-averaged F1 scores observed.\n",
    "\n",
    "* The models were not subjected to hyperparameter tuning, which offers another avenue for substantial enhancements in performance.\n",
    "\n",
    "* I would encourage further exploration and refinement, particularly in areas such as handling missing values, creating new derived features, and diving deeper into the data's intricacies. Building upon these insights could lead to model improvements and better predictive performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
